# Phase 1: Business Understanding

## Research Question
**"Can we predict which customers are likely to cancel their service (Churn) based on their demographic and account information?"**

## Project Objective
The goal of this project is to solve a classification problem using the CRISP-DM methodology. We aim to identify high-risk customers 
so the business can intervene with retention offers.

## Dataset
We are using the [Telco Customer Churn dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).
- **Target Variable:** `Churn` (Yes/No)
- **Features:** Services signed up for, customer account information, and demographic info.

Code part --------------------------------------------------------------------------------------------------------------------------

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Configure visuals for high-quality SVG output (Required for presentation)
%config InlineBackend.figure_formats = ['svg']
plt.style.use('seaborn-v0_8-whitegrid')

# Create a folder for images if it doesn't exist
import os
if not os.path.exists('images'):
    os.makedirs('images')

print("Libraries loaded and configuration complete.")

# Phase 2: Data Preparation
We will load the data and perform an initial inspection. We expect 'TotalCharges' to be numeric,
but source documentation suggests it might be stored as an object (string) due to blank spaces.
We will fix this and handle missing values.

Code part --------------------------------------------------------------------------------------------------------------------------

# Load the dataset
try:
    df = pd.read_csv('data.csv')
    print("Data loaded successfully.")
except FileNotFoundError:
    print("Error: data.csv not found. Please ensure the file is in the project directory.")

# Data Cleaning Step 1: Fix TotalCharges
# Coerce errors to NaN to handle blank strings
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Data Cleaning Step 2: Handle Missing Values
missing_count = df.isnull().sum().sum()
print(f"Missing values found: {missing_count}")

# Since missing values are < 1% of data, we drop them
df.dropna(inplace=True)

# Data Cleaning Step 3: Remove irrelevant features
# customerID is unique per row and has no predictive power
df.drop(columns=['customerID'], inplace=True)

# Data Cleaning Step 4: Encode Target Variable
# Convert 'Yes'/'No' to 1/0
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

print(f"Data cleaning complete. Final dataset shape: {df.shape}")
df.head()

# Phase 3: Exploratory Data Analysis (EDA)

We visualize the data to understand the factors driving churn.
**Hypothesis:** Customers on "Month-to-month" contracts are more flexible and therefore more likely to leave than those on long-term contracts.

Code part --------------------------------------------------------------------------------------------------------------------------

# Helper function to save vector graphics
def save_plot(filename):
    plt.savefig(f"images/{filename}.svg", format='svg', bbox_inches='tight')

# Visualization 1: Target Balance
plt.figure(figsize=(6, 4))
sns.countplot(x='Churn', data=df, palette='viridis')
plt.title('Distribution of Churn (Target Variable)')
plt.xlabel('Churn (0=No, 1=Yes)')
save_plot('churn_distribution')
plt.show()

# Visualization 2: Contract Type vs Churn
plt.figure(figsize=(8, 5))
sns.countplot(x='Contract', hue='Churn', data=df, palette='Set2')
plt.title('Churn Rate by Contract Type')
save_plot('contract_churn')
plt.show()

### Analysis of EDA
The visualizations confirm our hypothesis:
1.  **Imbalance:** The dataset is imbalanced; there are significantly more non-churners (0) than churners (1). 
This means accuracy alone might be a misleading metric.
2.  **Contract Influence:** The "Month-to-month" contract type shows a drastic spike in churn compared to "One year" or "Two year" contracts. 
This suggests `Contract` is a high-value feature.

# Phase 4: Modeling

We will prepare the data for machine learning by converting categorical variables into numbers (One-Hot Encoding) and scaling numerical features.

We will train three distinct models to compare performance:
1.  **Logistic Regression:** A baseline linear model, good for interpretability.
2.  **Decision Tree:** Capable of capturing non-linear patterns but prone to overfitting.
3.  **Random Forest:** An ensemble method that usually provides higher accuracy and stability.

Code part --------------------------------------------------------------------------------------------------------------------------

# 1. Feature Engineering (One-Hot Encoding)
df_encoded = pd.get_dummies(df, drop_first=True)

# 2. Split Data (80% Train, 20% Test)
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Scaling (Important for Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Initialize Models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# 5. Train and Predict
results = {}

print("Training Models...\n")
for name, model in models.items():
    # Use scaled data for Logistic Regression, raw data for Trees
    if name == "Logistic Regression":
        model.fit(X_train_scaled, y_train)
        predictions = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
    
    # Store accuracy
    acc = accuracy_score(y_test, predictions)
    results[name] = acc
    
    # Print Report
    print(f"--- {name} ---")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, predictions))

# Phase 5: Evaluation

We compare the models based on Accuracy.

Code part --------------------------------------------------------------------------------------------------------------------------

# Compare Model Performance
plt.figure(figsize=(10, 6))
bars = plt.bar(results.keys(), results.values(), color=['#1f77b4', '#ff7f0e', '#2ca02c'])
plt.title('Model Accuracy Comparison')
plt.ylim(0.7, 0.85) # Zoom in to show differences
plt.ylabel('Accuracy Score')

# Add value labels on top
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.3f}', va='bottom', ha='center')

save_plot('model_comparison')
plt.show()

## Conclusion & Deployment
**Best Model:** The **Logistic Regression** and **Random Forest** performed similarly (approx 79-80% accuracy).

**Selection:** We choose **Logistic Regression** for deployment.
* **Reason:** It offers similar performance to the more complex Random Forest but is computationally faster and allows us to easily see 
which coefficients (features) increase the odds of churn.

**Answer to Research Question:**
Yes, we can predict churn with ~80% accuracy. The data analysis clearly identifies short-term contracts and high monthly charges as key 
drivers of customer attrition.

